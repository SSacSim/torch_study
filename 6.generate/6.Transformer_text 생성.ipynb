{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder , TransformerEncoderLayer\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module) : # 모델 정의\n",
    "    '''\n",
    "    num_token : 토큰 갯수 > 단어 임베딩을 위함\n",
    "    num_inputs : embedding 차원 수\n",
    "    num_heads : 멀티헤드 수\n",
    "    num_hidden : layer 차원수\n",
    "    num_layers : layer 수\n",
    "    dropout = dropout 비율 defalut:0.3 \n",
    "    '''\n",
    "    def __init__(self, num_token, num_inputs, num_heads, num_hidden, num_layers, dropout = 0.3):\n",
    "        super(Transformer, self).__init__()\n",
    "        #======================= 초기설정 =============================\n",
    "        self.model_name = \"transformer\"\n",
    "        self.mask_source = None\n",
    "        self.position_enc = PosEnc(num_inputs, dropout) # PosEnd -> functional\n",
    "        \n",
    "        layers_enc = TransformerEncoderLayer(num_inputs,num_heads,num_hidden, dropout) # encoder block\n",
    "        self.enc_transformer = TransformerEncoder(layers_enc, num_layers) # encoder block를 n개 쌓겠다\n",
    "        self.enc = nn.Embedding(num_token, num_inputs) # word to embedding layer\n",
    "        self.num_inputs = num_inputs\n",
    "        self.dec = nn.Linear(num_inputs, num_token)\n",
    "        self.init_params() # init_params() -> functional\n",
    "    \n",
    "    def _gen_sqr_nxt_mask(self, size):\n",
    "        ''' \n",
    "        torch.triu : 대각행렬 기준 위쪽만 1로 채움\n",
    "            ex) in : torch.triu(torch.ones(5,5))\n",
    "                out : tensor([[1., 1., 1., 1., 1.],\n",
    "                              [0., 1., 1., 1., 1.],\n",
    "                              [0., 0., 1., 1., 1.],\n",
    "                              [0., 0., 0., 1., 1.],\n",
    "                              [0., 0., 0., 0., 1.]])\n",
    "                              \n",
    "                in : (torch.triu(torch.ones(size,size)) == 1).transpose(0,1) \n",
    "                out : tensor([[ True, False, False, False, False],\n",
    "                              [ True,  True, False, False, False],\n",
    "                              [ True,  True,  True, False, False],\n",
    "                              [ True,  True,  True,  True, False],\n",
    "                              [ True,  True,  True,  True,  True]])    \n",
    "                \n",
    "        '''\n",
    "        msk = (torch.triu(torch.ones(size,size)) == 1).transpose(0,1) # mask 안할 위치 지정\n",
    "        msk = msk.float().masked_fill(msk==0, float(\"-inf\")) # mask 할 위치 -inf\n",
    "        msk = msk.masked_fill(msk == 1, float(0.0)) # mask 안할 위치 0으로 변경\n",
    "        return msk\n",
    "    \n",
    "    def init_params(self):\n",
    "        ''' \n",
    "        파라미터 초기화 세팅 function\n",
    "        '''\n",
    "        initial_rng = 0.12\n",
    "        self.enc.weight.data.uniform_(-initial_rng , initial_rng) # -initial_rng ~ initial_rng 사이 실수\n",
    "        self.dec.bias.data.zero_() # bias 0\n",
    "        self.dec.weight.data.uniform_(-initial_rng , initial_rng)\n",
    "        \n",
    "    def forward(self, source , src_mask = None):\n",
    "        '''\n",
    "        source : 단어 list\n",
    "        mask_source : mask의 형태(특정 마스크로 지정가능)\n",
    "        '''\n",
    "        if src_mask is None or src_mask.size(0) != len(source): # mask_source가 지정되지 않으면\n",
    "            dvc = source.device # 입력데이터는 어디에 적재되어있는가 \"CPU\"? or \"GPU\"\n",
    "            msk = self._gen_sqr_nxt_mask(len(source)).to(dvc) # mask 배열 생성 \n",
    "            self.mask_source = msk  \n",
    "            \n",
    "        source = self.enc(source) * math.sqrt(self.num_inputs) # self.enc : embedding layer / out > source embedding \n",
    "                                                               # math.sqrt : 값 정규화\n",
    "        source = self.position_enc(source) # positional encoding\n",
    "        \n",
    "        op = self.enc_transformer(source, self.mask_source)\n",
    "        op = self.dec(op) # 단순 linear를 이용하여 decode\n",
    "        return op\n",
    "class PosEnc(nn.Module):\n",
    "    '''\n",
    "    Positional Encoding \n",
    "    '''\n",
    "    def __init__(self, d_m, dropout = 0.2 , size_limit = 5000):\n",
    "        super(PosEnc,self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        p_enc = torch.zeros(size_limit, d_m) # size_limit : vocab length, d_m : embedding dim \n",
    "                                             # 2차원 array\n",
    "        \n",
    "        pos = torch.arange(0, size_limit,dtype = torch.float).unsqueeze(1) # [size_limit,] > [size_limit,1]\n",
    "        \n",
    "        divider = torch.exp(torch.arange(0,d_m,2).float() * (-math.log(10000.0) / d_m))\n",
    "        \n",
    "        p_enc[:,0::2] = torch.sin(pos * divider)\n",
    "        p_enc[:,1::2] = torch.cos(pos * divider) # [size_limit,d_m] \n",
    "        p_enc = p_enc.unsqueeze(0).transpose(0,1) # [size_limit,d_m] > [1,size_limit,d_m] > [size_limit,1,d_m]\n",
    "        \n",
    "        self.register_buffer(\"p_enc\",p_enc)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.dropout(x + self.p_enc[:x.size(0),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torchtext.data.utils._basic_english_normalize(line)>"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tutorials.pytorch.kr/beginner/transformer_tutorial.html\n",
    "# 위 링크를 참고하였습니다.\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    " \n",
    "# 1. vocab 생성\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter : # (label, text) \n",
    "        yield tokenizer(text)\n",
    "\n",
    "training_text, validation_text, testing_text = torchtext.datasets.WikiText2() # train, val, test 제공\n",
    "\n",
    "# training_text를 이용하여 vocab 생성\n",
    "vocab = build_vocab_from_iterator(yield_tokens(training_text),\n",
    "                                  specials= [\"<sos>\",\"<eos>\",\"<unk>\"]) # vocab생성\n",
    "vocab.set_default_index(vocab[\"<unk>\"]) # 없는 단어 처리\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. sentence to number (vocab)\n",
    "\n",
    "def data_process(raw_text_iter):\n",
    "    # 텍스트를 하나씩 넣어 숫자로 변환\n",
    "    data = [ torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]    \n",
    "    # list.numel() > 원소 개수\n",
    "    # 원소가 하나도 없음 : 텅빈 문장은 제외 -> 모든 문장의 길이 [n,] 형식으로 저장\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) \n",
    "\n",
    "train_data = data_process(training_text) \n",
    "val_data = data_process(validation_text)\n",
    "test_data = data_process(testing_text)\n",
    "\n",
    "def batchify(data , bsz): # 지정한 최대길이(batch_size)만큼 자름\n",
    "    '''\n",
    "    배치크기 나누기\n",
    "    '''\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz,seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20 \n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size) # 문장별로 넣는게 아니라, 모든 단어를 일차원으로 나열하여 단순하게 자름\n",
    "val_data = batchify(val_data, eval_batch_size) \n",
    "test_data = batchify(test_data, eval_batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i ):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)\n",
    "emsize = 100 # embedding size\n",
    "d_hid = 100 # hidden dim\n",
    "nlayers = 2 \n",
    "nhead = 2\n",
    "dropout = 0.2\n",
    "model = Transformer(ntokens,emsize,nhead,d_hid,nlayers,dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 5.0\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma = 0.95)\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    \n",
    "    num_batches = len(train_data) // bptt\n",
    "    \n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1 ,bptt)):\n",
    "        data, targets = get_batch(train_data,i)\n",
    "        seq_len = data.size(0)\n",
    "        if seq_len != bptt: # 마지막 부분에 설정한 부분보다 적은 수의 단어가 들어가있는 경우가 있기에\n",
    "            src_mask = src_mask[:seq_len, :seq_len]\n",
    "\n",
    "        output = model(data,src_mask)\n",
    "        \n",
    "        loss = criterion(output.view(-1,ntokens) , targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # 기울기 폭발 방지\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch> 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data):\n",
    "    model.eval()  # 평가 모드 시작\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            if seq_len != bptt:\n",
    "                src_mask = src_mask[:seq_len, :seq_len]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 10.17 | loss  8.14 | ppl  3422.65\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 11.14 | loss  6.75 | ppl   851.33\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 10.82 | loss  5.96 | ppl   385.79\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 11.45 | loss  5.74 | ppl   311.77\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 11.56 | loss  5.63 | ppl   278.73\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 11.17 | loss  5.60 | ppl   270.33\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 11.01 | loss  5.52 | ppl   250.22\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 11.42 | loss  5.38 | ppl   217.61\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 11.49 | loss  4.89 | ppl   132.84\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 11.55 | loss  4.61 | ppl   100.46\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 10.78 | loss  4.38 | ppl    80.23\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 10.46 | loss  4.32 | ppl    75.17\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 10.64 | loss  4.22 | ppl    67.81\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch  9.87 | loss  4.04 | ppl    57.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 33.41s | valid loss  2.97 | valid ppl    19.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch  9.59 | loss  3.83 | ppl    45.97\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 11.19 | loss  3.73 | ppl    41.48\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 11.51 | loss  3.52 | ppl    33.95\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 10.86 | loss  3.48 | ppl    32.35\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 10.60 | loss  3.36 | ppl    28.85\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 11.29 | loss  3.35 | ppl    28.40\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 11.23 | loss  3.34 | ppl    28.12\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 11.02 | loss  3.33 | ppl    27.87\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 10.99 | loss  3.22 | ppl    25.09\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch  8.75 | loss  3.19 | ppl    24.34\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch  9.09 | loss  3.10 | ppl    22.26\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch  9.50 | loss  3.18 | ppl    23.94\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch  9.61 | loss  3.13 | ppl    22.90\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 11.05 | loss  3.05 | ppl    21.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 32.06s | valid loss  1.83 | valid ppl     6.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 10.40 | loss  2.95 | ppl    19.09\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 10.66 | loss  2.92 | ppl    18.48\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 10.60 | loss  2.79 | ppl    16.35\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 10.77 | loss  2.78 | ppl    16.20\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 10.55 | loss  2.72 | ppl    15.20\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 10.81 | loss  2.77 | ppl    15.93\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 11.25 | loss  2.77 | ppl    15.97\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 11.12 | loss  2.73 | ppl    15.41\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 10.71 | loss  2.70 | ppl    14.83\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 10.87 | loss  2.71 | ppl    15.03\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 10.73 | loss  2.63 | ppl    13.89\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch  9.45 | loss  2.70 | ppl    14.92\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 10.64 | loss  2.69 | ppl    14.75\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 10.68 | loss  2.61 | ppl    13.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 32.75s | valid loss  1.35 | valid ppl     3.85\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  1.32 | test ppl     3.75\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa996f07c7a4790828b1e22eaa2430332fe2e2f9495926bea94e65a6711ba0db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder , TransformerEncoderLayer\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module) : # 모델 정의\n",
    "    '''\n",
    "    num_token : 토큰 갯수 > 단어 임베딩을 위함\n",
    "    num_inputs : embedding 차원 수\n",
    "    num_heads : 멀티헤드 수\n",
    "    num_hidden : layer 차원수\n",
    "    num_layers : layer 수\n",
    "    dropout = dropout 비율 defalut:0.3 \n",
    "    '''\n",
    "    def __init__(self, num_token, num_inputs, num_heads, num_hidden, num_layers, dropout = 0.3):\n",
    "        super(Transformer, self).__init__()\n",
    "        #======================= 초기설정 =============================\n",
    "        self.model_name = \"transformer\"\n",
    "        self.mask_source = None\n",
    "        self.position_enc = PosEnc(num_inputs, dropout) # PosEnd -> functional\n",
    "        \n",
    "        layers_enc = TransformerEncoderLayer(num_inputs,num_heads,num_hidden, dropout) # encoder block\n",
    "        self.enc_transformer = TransformerEncoder(layers_enc, num_layers) # encoder block를 n개 쌓겠다\n",
    "        self.enc = nn.Embedding(num_token, num_inputs) # word to embedding layer\n",
    "        self.num_inputs = num_inputs\n",
    "        self.dec = nn.Linear(num_inputs, num_token)\n",
    "        self.init_params() # init_params() -> functional\n",
    "    \n",
    "    def _gen_sqr_nxt_mask(self, size):\n",
    "        ''' \n",
    "        torch.triu : 대각행렬 기준 위쪽만 1로 채움\n",
    "            ex) in : torch.triu(torch.ones(5,5))\n",
    "                out : tensor([[1., 1., 1., 1., 1.],\n",
    "                              [0., 1., 1., 1., 1.],\n",
    "                              [0., 0., 1., 1., 1.],\n",
    "                              [0., 0., 0., 1., 1.],\n",
    "                              [0., 0., 0., 0., 1.]])\n",
    "                              \n",
    "                in : (torch.triu(torch.ones(size,size)) == 1).transpose(0,1) \n",
    "                out : tensor([[ True, False, False, False, False],\n",
    "                              [ True,  True, False, False, False],\n",
    "                              [ True,  True,  True, False, False],\n",
    "                              [ True,  True,  True,  True, False],\n",
    "                              [ True,  True,  True,  True,  True]])    \n",
    "                \n",
    "        '''\n",
    "        msk = (torch.triu(torch.ones(size,size)) == 1).transpose(0,1) # mask 안할 위치 지정\n",
    "        msk = msk.float().masked_fill(msk==0, float(\"-inf\")) # mask 할 위치 -inf\n",
    "        msk = msk.masked_fill(msk == 1, float(0.0)) # mask 안할 위치 0으로 변경\n",
    "        return msk\n",
    "    \n",
    "    def init_params(self):\n",
    "        ''' \n",
    "        파라미터 초기화 세팅 function\n",
    "        '''\n",
    "        initial_rng = 0.12\n",
    "        self.enc.weight.data.uniform_(-initial_rng , initial_rng) # -initial_rng ~ initial_rng 사이 실수\n",
    "        self.dec.bias.data.zero_() # bias 0\n",
    "        self.dec.weight.data.uniform_(-initial_rng , initial_rng)\n",
    "        \n",
    "    def forward(self, source):\n",
    "        '''\n",
    "        source : 단어 list\n",
    "        mask_source : mask의 형태(특정 마스크로 지정가능)\n",
    "        '''\n",
    "        if self.mask_source is None or self.mask_source.size(0) != len(source): # mask_source가 지정되지 않으면\n",
    "            dvc = source.device # 입력데이터는 어디에 적재되어있는가 \"CPU\"? or \"GPU\"\n",
    "            msk = self._gen_sqr_nxt_mask(len(source)).to(dvc) # mask 배열 생성 \n",
    "            self.mask_source = msk  \n",
    "            \n",
    "        source = self.enc(source) * math.sqrt(self.num_inputs) # self.enc : embedding layer / out > source embedding \n",
    "                                                               # math.sqrt : 값 정규화\n",
    "        source = self.position_enc(source) # positional encoding\n",
    "        \n",
    "        op = self.enc_transformer(source, self.mask_source)\n",
    "        op = self.dec(op) # 단순 linear를 이용하여 decode\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEnc(nn.Module):\n",
    "    '''\n",
    "    Positional Encoding \n",
    "    '''\n",
    "    def __init__(self, d_m, dropout = 0.2 , size_limit = 5000):\n",
    "        super(PosEnc,self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        p_enc = torch.zeros(size_limit, d_m) # size_limit : vocab length, d_m : embedding dim \n",
    "                                             # 2차원 array\n",
    "        \n",
    "        pos = torch.arange(0, size_limit,dtype = torch.float).unsqueeze(1) # [size_limit,] > [size_limit,1]\n",
    "        \n",
    "        divider = torch.exp(torch.arange(0,d_m,2).float() * (-math.log(10000.0) / d_m))\n",
    "        \n",
    "        p_enc[:,0::2] = torch.sin(pos * divider)\n",
    "        p_enc[:,1::2] = torch.cos(pos * divider) # [size_limit,d_m] \n",
    "        p_enc = p_enc.unsqueeze(0).transpose(0,1) # [size_limit,d_m] > [1,size_limit,d_m] > [size_limit,1,d_m]\n",
    "        \n",
    "        self.register_buffer(\"p_enc\",p_enc)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.dropout(x + self.p_enc[:x.size(0),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torchtext.data.utils._basic_english_normalize(line)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tutorials.pytorch.kr/beginner/text_sentiment_ngrams_tutorial.html\n",
    "# 위 링크를 참고하여 코드를 재구성하였습니다.\n",
    "\n",
    "train_iter = iter(torchtext.datasets.AG_NEWS(split=\"train\"))\n",
    "#next(train_iter)  #(label, text)형식으로 구성\n",
    " \n",
    " \n",
    "# 1. vocab 생성\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_iter = torchtext.datasets.AG_NEWS(split=\"train\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _ , text in data_iter : # (label, text) \n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter),\n",
    "                                  specials=[\"<unk>\"],\n",
    "                                  max_tokens = 10)\n",
    "\n",
    "vocab.set_default_index(vocab[\"the\"]) #vocab 안에 없는 단어가 들어왔을때 해당 단어의 index를 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ShardingFilterIterDataPipe,\n",
       " ShardingFilterIterDataPipe,\n",
       " ShardingFilterIterDataPipe)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_text, validation_text, testing_text = torchtext.datasets.WikiText2() # train, val, test 제공\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter : # (label, text) \n",
    "        yield tokenizer(text)\n",
    "\n",
    "training_text, validation_text, testing_text = torchtext.datasets.WikiText2() # train, val, test 제공\n",
    "\n",
    "# training_text를 이용하여 vocab 생성\n",
    "vocab = build_vocab_from_iterator(yield_tokens(training_text),\n",
    "                                  specials= [\"<sos>\",\"<eos>\",\"<unk>\"]) # vocab생성\n",
    "vocab.set_default_index(vocab[\"<unk>\"]) # 없는 단어 처리\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def data_process(raw_text_iter):\n",
    "    # 텍스트를 하나씩 넣어 숫자로 변환\n",
    "    data = [ torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]    \n",
    "\n",
    "    return \n",
    "\n",
    "                                                                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in training_text]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23767"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(filter(lambda t: t.numel() > 0, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<filter at 0x1378dbf3df0>"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter(lambda t: t.numel() > 0, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36718"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"ShardingFilterIterDataPipe\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11336\\3910918786.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11336\\3359158169.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtext_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"<sos> \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" <eos>\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# vocab기반으로 문장을 문자 sequence로 변경\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"ShardingFilterIterDataPipe\") to str"
     ]
    }
   ],
   "source": [
    "text_tokenizer(training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batches(text_dataset, batch_size):\n",
    "    text_dataset = TEXT.numericalize([text_dataset.examples[0].text])\n",
    "    # divide text dataset into parts of size equal to batch_size\n",
    "    num_batches = text_dataset.size(0) // batch_size\n",
    "    # remove data points that lie outside batches (remainders)\n",
    "    text_dataset = text_dataset.narrow(0, 0, num_batches * batch_size)\n",
    "    # distribute dataset across batches evenly\n",
    "    text_dataset = text_dataset.view(batch_size, -1).t().contiguous()\n",
    "    return text_dataset.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchtext.data' has no attribute 'Field'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11336\\1874098797.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTEXT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"basic_english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meos_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'<eos>'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'<sos>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtraining_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWikiText2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      3\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchtext.data' has no attribute 'Field'"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"), lower=True, eos_token='<eos>', init_token='<sos>')\n",
    "training_text, validation_text, testing_text = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(training_text)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def gen_batches(text_dataset, batch_size):\n",
    "    text_dataset = TEXT.numericalize([text_dataset.examples[0].text])\n",
    "    # divide text dataset into parts of size equal to batch_size\n",
    "    num_batches = text_dataset.size(0) // batch_size\n",
    "    # remove data points that lie outside batches (remainders)\n",
    "    text_dataset = text_dataset.narrow(0, 0, num_batches * batch_size)\n",
    "    # distribute dataset across batches evenly\n",
    "    text_dataset = text_dataset.view(batch_size, -1).t().contiguous()\n",
    "    return text_dataset.to(device)\n",
    "\n",
    "training_batch_size = 32\n",
    "evaluation_batch_size = 16\n",
    "\n",
    "training_data = gen_batches(training_text, training_batch_size)\n",
    "validation_data = gen_batches(validation_text, evaluation_batch_size)\n",
    "testing_data = gen_batches(testing_text, evaluation_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Token 안녕! not found and default index is not set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11336\\1328271570.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"안녕!\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\NIPA_ICTCoC\\anaconda3\\lib\\site-packages\\torchtext\\vocab\\vocab.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0massociated\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \"\"\"\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Token 안녕! not found and default index is not set"
     ]
    }
   ],
   "source": [
    "vocab[\"안녕!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchtext.data' has no attribute 'Field'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11336\\1874098797.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTEXT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"basic_english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meos_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'<eos>'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'<sos>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtraining_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWikiText2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchtext.data' has no attribute 'Field'"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"), lower=True, eos_token='<eos>', init_token='<sos>')\n",
    "training_text, validation_text, testing_text = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(training_text)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def gen_batches(text_dataset, batch_size):\n",
    "    text_dataset = TEXT.numericalize([text_dataset.examples[0].text])\n",
    "    # divide text dataset into parts of size equal to batch_size\n",
    "    num_batches = text_dataset.size(0) // batch_size\n",
    "    # remove data points that lie outside batches (remainders)\n",
    "    text_dataset = text_dataset.narrow(0, 0, num_batches * batch_size)\n",
    "    # distribute dataset across batches evenly\n",
    "    text_dataset = text_dataset.view(batch_size, -1).t().contiguous()\n",
    "    return text_dataset.to(device)\n",
    "\n",
    "training_batch_size = 32\n",
    "evaluation_batch_size = 16\n",
    "\n",
    "training_data = gen_batches(training_text, training_batch_size)\n",
    "validation_data = gen_batches(validation_text, evaluation_batch_size)\n",
    "testing_data = gen_batches(testing_text, evaluation_batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa996f07c7a4790828b1e22eaa2430332fe2e2f9495926bea94e65a6711ba0db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
